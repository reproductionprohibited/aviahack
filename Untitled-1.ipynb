{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_index(features, k=5):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(features[:, :3])\n",
    "    _, indices = nbrs.kneighbors(features[:, :3])\n",
    "\n",
    "    edges = []\n",
    "    for i in range(len(features)):\n",
    "        for j in indices[i]:\n",
    "            if i != j:\n",
    "                edges.append([i, j])\n",
    "\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('data.csv')  # Замените на путь к вашему файлу CSV\n",
    "shuffled_data = shuffle(data_frame)\n",
    "reduced_data_frame = data_frame.iloc[::500, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(data_frame[['x', 'y', 'z', 'v_x', 'v_y', 'v_z']].values)\n",
    "target = data_frame['pressure'].values\n",
    "\n",
    "\n",
    "# Разделение данных на обучающий и валидационный наборы\n",
    "features_train, features_val, target_train, target_val = train_test_split(features, target, test_size=0.2)\n",
    "\n",
    "# Создание edge_index для обучающего и валидационного наборов\n",
    "edge_index_train = create_edge_index(features_train)\n",
    "edge_index_val = create_edge_index(features_val)\n",
    "\n",
    "# Преобразование данных в тензоры\n",
    "x_tensor_train = torch.tensor(features_train, dtype=torch.float)\n",
    "y_tensor_train = torch.tensor(target_train, dtype=torch.float).unsqueeze(1)\n",
    "x_tensor_val = torch.tensor(features_val, dtype=torch.float)\n",
    "y_tensor_val = torch.tensor(target_val, dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(6, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 64)\n",
    "        self.conv4 = GCNConv(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.leaky_relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.leaky_relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.leaky_relu(self.conv3(x, edge_index))\n",
    "        x = self.conv4(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 20\n",
    "early_stopping_counter = 0\n",
    "best_val_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training Loss: 9559824384.0, Validation Loss: 9556235264.0\n",
      "Epoch 1, Training Loss: 9559687168.0, Validation Loss: 9556075520.0\n",
      "Epoch 2, Training Loss: 9559523328.0, Validation Loss: 9555866624.0\n",
      "Epoch 3, Training Loss: 9559311360.0, Validation Loss: 9555586048.0\n",
      "Epoch 4, Training Loss: 9559033856.0, Validation Loss: 9555219456.0\n",
      "Epoch 5, Training Loss: 9558671360.0, Validation Loss: 9554744320.0\n",
      "Epoch 6, Training Loss: 9558208512.0, Validation Loss: 9554137088.0\n",
      "Epoch 7, Training Loss: 9557615616.0, Validation Loss: 9553374208.0\n",
      "Epoch 8, Training Loss: 9556875264.0, Validation Loss: 9552433152.0\n",
      "Epoch 9, Training Loss: 9555953664.0, Validation Loss: 9551279104.0\n",
      "Epoch 10, Training Loss: 9554829312.0, Validation Loss: 9549880320.0\n",
      "Epoch 11, Training Loss: 9553469440.0, Validation Loss: 9548203008.0\n",
      "Epoch 12, Training Loss: 9551832064.0, Validation Loss: 9546203136.0\n",
      "Epoch 13, Training Loss: 9549877248.0, Validation Loss: 9543835648.0\n",
      "Epoch 14, Training Loss: 9547568128.0, Validation Loss: 9541049344.0\n",
      "Epoch 15, Training Loss: 9544857600.0, Validation Loss: 9537784832.0\n",
      "Epoch 16, Training Loss: 9541661696.0, Validation Loss: 9533980672.0\n",
      "Epoch 17, Training Loss: 9537945600.0, Validation Loss: 9529565184.0\n",
      "Epoch 18, Training Loss: 9533650944.0, Validation Loss: 9524465664.0\n",
      "Epoch 19, Training Loss: 9528662016.0, Validation Loss: 9518597120.0\n",
      "Epoch 20, Training Loss: 9522940928.0, Validation Loss: 9511868416.0\n",
      "Epoch 21, Training Loss: 9516401664.0, Validation Loss: 9504181248.0\n",
      "Epoch 22, Training Loss: 9508897792.0, Validation Loss: 9495429120.0\n",
      "Epoch 23, Training Loss: 9500350464.0, Validation Loss: 9485495296.0\n",
      "Epoch 24, Training Loss: 9490632704.0, Validation Loss: 9474253824.0\n",
      "Epoch 25, Training Loss: 9479706624.0, Validation Loss: 9461573632.0\n",
      "Epoch 26, Training Loss: 9467272192.0, Validation Loss: 9447302144.0\n",
      "Epoch 27, Training Loss: 9453372416.0, Validation Loss: 9431287808.0\n",
      "Epoch 28, Training Loss: 9437769728.0, Validation Loss: 9413359616.0\n",
      "Epoch 29, Training Loss: 9420198912.0, Validation Loss: 9393341440.0\n",
      "Epoch 30, Training Loss: 9400787968.0, Validation Loss: 9371038720.0\n",
      "Epoch 31, Training Loss: 9378886656.0, Validation Loss: 9346253824.0\n",
      "Epoch 32, Training Loss: 9354771456.0, Validation Loss: 9318767616.0\n",
      "Epoch 33, Training Loss: 9328086016.0, Validation Loss: 9288352768.0\n",
      "Epoch 34, Training Loss: 9298404352.0, Validation Loss: 9254768640.0\n",
      "Epoch 35, Training Loss: 9265575936.0, Validation Loss: 9217760256.0\n",
      "Epoch 36, Training Loss: 9229244416.0, Validation Loss: 9177062400.0\n",
      "Epoch 37, Training Loss: 9189738496.0, Validation Loss: 9132396544.0\n",
      "Epoch 38, Training Loss: 9146008576.0, Validation Loss: 9083472896.0\n",
      "Epoch 39, Training Loss: 9098249216.0, Validation Loss: 9029990400.0\n",
      "Epoch 40, Training Loss: 9046041600.0, Validation Loss: 8971636736.0\n",
      "Epoch 41, Training Loss: 8988893184.0, Validation Loss: 8908091392.0\n",
      "Epoch 42, Training Loss: 8926097408.0, Validation Loss: 8839026688.0\n",
      "Epoch 43, Training Loss: 8859484160.0, Validation Loss: 8764112896.0\n",
      "Epoch 44, Training Loss: 8785939456.0, Validation Loss: 8683009024.0\n",
      "Epoch 45, Training Loss: 8706741248.0, Validation Loss: 8595373056.0\n",
      "Epoch 46, Training Loss: 8621234176.0, Validation Loss: 8500867072.0\n",
      "Epoch 47, Training Loss: 8528542720.0, Validation Loss: 8399151616.0\n",
      "Epoch 48, Training Loss: 8429007360.0, Validation Loss: 8289893376.0\n",
      "Epoch 49, Training Loss: 8322222080.0, Validation Loss: 8172771840.0\n",
      "Epoch 50, Training Loss: 8207728128.0, Validation Loss: 8047477760.0\n",
      "Epoch 51, Training Loss: 8085373440.0, Validation Loss: 7913722880.0\n",
      "Epoch 52, Training Loss: 7954337280.0, Validation Loss: 7771241984.0\n",
      "Epoch 53, Training Loss: 7813798400.0, Validation Loss: 7619798528.0\n",
      "Epoch 54, Training Loss: 7666031616.0, Validation Loss: 7459192832.0\n",
      "Epoch 55, Training Loss: 7509541376.0, Validation Loss: 7289273344.0\n",
      "Epoch 56, Training Loss: 7341166592.0, Validation Loss: 7109931008.0\n",
      "Epoch 57, Training Loss: 7164102144.0, Validation Loss: 6921120768.0\n",
      "Epoch 58, Training Loss: 6980565504.0, Validation Loss: 6722865152.0\n",
      "Epoch 59, Training Loss: 6784836096.0, Validation Loss: 6515263488.0\n",
      "Epoch 60, Training Loss: 6580593152.0, Validation Loss: 6298501632.0\n",
      "Epoch 61, Training Loss: 6365500928.0, Validation Loss: 6072859136.0\n",
      "Epoch 62, Training Loss: 6143768576.0, Validation Loss: 5838737920.0\n",
      "Epoch 63, Training Loss: 5911509504.0, Validation Loss: 5596659200.0\n",
      "Epoch 64, Training Loss: 5671375872.0, Validation Loss: 5347276800.0\n",
      "Epoch 65, Training Loss: 5423098880.0, Validation Loss: 5091394560.0\n",
      "Epoch 66, Training Loss: 5169991680.0, Validation Loss: 4829979648.0\n",
      "Epoch 67, Training Loss: 4912836608.0, Validation Loss: 4564180992.0\n",
      "Epoch 68, Training Loss: 4647315456.0, Validation Loss: 4295315968.0\n",
      "Epoch 69, Training Loss: 4379201536.0, Validation Loss: 4024899328.0\n",
      "Epoch 70, Training Loss: 4107335424.0, Validation Loss: 3754624768.0\n",
      "Epoch 71, Training Loss: 3835084032.0, Validation Loss: 3486394368.0\n",
      "Epoch 72, Training Loss: 3565220096.0, Validation Loss: 3222283520.0\n",
      "Epoch 73, Training Loss: 3295711232.0, Validation Loss: 2964561408.0\n",
      "Epoch 74, Training Loss: 3032003072.0, Validation Loss: 2715604480.0\n",
      "Epoch 75, Training Loss: 2772278528.0, Validation Loss: 2477918720.0\n",
      "Epoch 76, Training Loss: 2532592640.0, Validation Loss: 2254083840.0\n",
      "Epoch 77, Training Loss: 2302172416.0, Validation Loss: 2046655232.0\n",
      "Epoch 78, Training Loss: 2088140288.0, Validation Loss: 1858061312.0\n",
      "Epoch 79, Training Loss: 1882624000.0, Validation Loss: 1690514176.0\n",
      "Epoch 80, Training Loss: 1703136640.0, Validation Loss: 1545944960.0\n",
      "Epoch 81, Training Loss: 1546070272.0, Validation Loss: 1425769344.0\n",
      "Epoch 82, Training Loss: 1412170368.0, Validation Loss: 1330722816.0\n",
      "Epoch 83, Training Loss: 1301442688.0, Validation Loss: 1260702336.0\n",
      "Epoch 84, Training Loss: 1214004864.0, Validation Loss: 1214594432.0\n",
      "Epoch 85, Training Loss: 1148010240.0, Validation Loss: 1190201472.0\n",
      "Epoch 86, Training Loss: 1102532096.0, Validation Loss: 1184292992.0\n",
      "Epoch 87, Training Loss: 1083867520.0, Validation Loss: 1192668544.0\n",
      "Epoch 88, Training Loss: 1083574528.0, Validation Loss: 1210469632.0\n",
      "Epoch 89, Training Loss: 1075267840.0, Validation Loss: 1232766848.0\n",
      "Epoch 90, Training Loss: 1080732160.0, Validation Loss: 1254828032.0\n",
      "Epoch 91, Training Loss: 1092205696.0, Validation Loss: 1272508288.0\n",
      "Epoch 92, Training Loss: 1100559232.0, Validation Loss: 1282713472.0\n",
      "Epoch 93, Training Loss: 1118610176.0, Validation Loss: 1283365504.0\n",
      "Epoch 94, Training Loss: 1110627456.0, Validation Loss: 1273908608.0\n",
      "Epoch 95, Training Loss: 1096343936.0, Validation Loss: 1254867712.0\n",
      "Epoch 96, Training Loss: 1083648000.0, Validation Loss: 1227532928.0\n",
      "Epoch 97, Training Loss: 1059945856.0, Validation Loss: 1193794432.0\n",
      "Epoch 98, Training Loss: 1031270912.0, Validation Loss: 1174509056.0\n",
      "Epoch 99, Training Loss: 1014390656.0, Validation Loss: 1153333120.0\n",
      "Epoch 100, Training Loss: 998402944.0, Validation Loss: 1130950144.0\n",
      "Epoch 101, Training Loss: 979863680.0, Validation Loss: 1107988608.0\n",
      "Epoch 102, Training Loss: 974634688.0, Validation Loss: 1084943488.0\n",
      "Epoch 103, Training Loss: 954056256.0, Validation Loss: 1062296256.0\n",
      "Epoch 104, Training Loss: 924884032.0, Validation Loss: 1040478720.0\n",
      "Epoch 105, Training Loss: 918622720.0, Validation Loss: 1019729024.0\n",
      "Epoch 106, Training Loss: 903116032.0, Validation Loss: 1000272768.0\n",
      "Epoch 107, Training Loss: 884944896.0, Validation Loss: 982237568.0\n",
      "Epoch 108, Training Loss: 876626944.0, Validation Loss: 965673472.0\n",
      "Epoch 109, Training Loss: 859371200.0, Validation Loss: 950601152.0\n",
      "Epoch 110, Training Loss: 851698880.0, Validation Loss: 936962944.0\n",
      "Epoch 111, Training Loss: 842360640.0, Validation Loss: 924690496.0\n",
      "Epoch 112, Training Loss: 837182336.0, Validation Loss: 913676736.0\n",
      "Epoch 113, Training Loss: 828535104.0, Validation Loss: 903812736.0\n",
      "Epoch 114, Training Loss: 822075328.0, Validation Loss: 894981120.0\n",
      "Epoch 115, Training Loss: 817445568.0, Validation Loss: 887054080.0\n",
      "Epoch 116, Training Loss: 818622272.0, Validation Loss: 879899904.0\n",
      "Epoch 117, Training Loss: 809184704.0, Validation Loss: 873412160.0\n",
      "Epoch 118, Training Loss: 802944384.0, Validation Loss: 867483392.0\n",
      "Epoch 119, Training Loss: 798893440.0, Validation Loss: 862013568.0\n",
      "Epoch 120, Training Loss: 796567616.0, Validation Loss: 856921920.0\n",
      "Epoch 121, Training Loss: 793585792.0, Validation Loss: 852133312.0\n",
      "Epoch 122, Training Loss: 793784640.0, Validation Loss: 847582976.0\n",
      "Epoch 123, Training Loss: 789118016.0, Validation Loss: 843225728.0\n",
      "Epoch 124, Training Loss: 786155584.0, Validation Loss: 839023488.0\n",
      "Epoch 125, Training Loss: 779859136.0, Validation Loss: 834952128.0\n",
      "Epoch 126, Training Loss: 781941632.0, Validation Loss: 830988288.0\n",
      "Epoch 127, Training Loss: 773406336.0, Validation Loss: 827119168.0\n",
      "Epoch 128, Training Loss: 771248960.0, Validation Loss: 823337280.0\n",
      "Epoch 129, Training Loss: 766446912.0, Validation Loss: 819641600.0\n",
      "Epoch 130, Training Loss: 765833024.0, Validation Loss: 816020416.0\n",
      "Epoch 131, Training Loss: 761362688.0, Validation Loss: 812474880.0\n",
      "Epoch 132, Training Loss: 752375104.0, Validation Loss: 809014976.0\n",
      "Epoch 133, Training Loss: 754075072.0, Validation Loss: 805637632.0\n",
      "Epoch 134, Training Loss: 754654656.0, Validation Loss: 802340352.0\n",
      "Epoch 135, Training Loss: 746989440.0, Validation Loss: 799126848.0\n",
      "Epoch 136, Training Loss: 745511040.0, Validation Loss: 795994048.0\n",
      "Epoch 137, Training Loss: 741181248.0, Validation Loss: 792943616.0\n",
      "Epoch 138, Training Loss: 738410816.0, Validation Loss: 789969600.0\n",
      "Epoch 139, Training Loss: 732400960.0, Validation Loss: 787069824.0\n",
      "Epoch 140, Training Loss: 730368512.0, Validation Loss: 784243008.0\n",
      "Epoch 141, Training Loss: 725610816.0, Validation Loss: 781482880.0\n",
      "Epoch 142, Training Loss: 721753920.0, Validation Loss: 778783232.0\n",
      "Epoch 143, Training Loss: 718318208.0, Validation Loss: 776134208.0\n",
      "Epoch 144, Training Loss: 716393728.0, Validation Loss: 773521088.0\n",
      "Epoch 145, Training Loss: 714538816.0, Validation Loss: 770939520.0\n",
      "Epoch 146, Training Loss: 707739584.0, Validation Loss: 768382720.0\n",
      "Epoch 147, Training Loss: 702906688.0, Validation Loss: 765845760.0\n",
      "Epoch 148, Training Loss: 698575360.0, Validation Loss: 763322752.0\n",
      "Epoch 149, Training Loss: 705505216.0, Validation Loss: 760788672.0\n",
      "Epoch 150, Training Loss: 697140352.0, Validation Loss: 758246464.0\n",
      "Epoch 151, Training Loss: 697394944.0, Validation Loss: 755684032.0\n",
      "Epoch 152, Training Loss: 692395456.0, Validation Loss: 753110976.0\n",
      "Epoch 153, Training Loss: 691836480.0, Validation Loss: 750512512.0\n",
      "Epoch 154, Training Loss: 684060288.0, Validation Loss: 747900800.0\n",
      "Epoch 155, Training Loss: 684854016.0, Validation Loss: 745263040.0\n",
      "Epoch 156, Training Loss: 680598400.0, Validation Loss: 742607488.0\n",
      "Epoch 157, Training Loss: 679360128.0, Validation Loss: 739930496.0\n",
      "Epoch 158, Training Loss: 675962624.0, Validation Loss: 737235520.0\n",
      "Epoch 159, Training Loss: 674553344.0, Validation Loss: 734519680.0\n",
      "Epoch 160, Training Loss: 670020480.0, Validation Loss: 731795456.0\n",
      "Epoch 161, Training Loss: 669359744.0, Validation Loss: 729058752.0\n",
      "Epoch 162, Training Loss: 664108544.0, Validation Loss: 726317568.0\n",
      "Epoch 163, Training Loss: 665267008.0, Validation Loss: 723568512.0\n",
      "Epoch 164, Training Loss: 659976512.0, Validation Loss: 720822080.0\n",
      "Epoch 165, Training Loss: 658995136.0, Validation Loss: 718081600.0\n",
      "Epoch 166, Training Loss: 652418688.0, Validation Loss: 715352832.0\n",
      "Epoch 167, Training Loss: 654196608.0, Validation Loss: 712636224.0\n",
      "Epoch 168, Training Loss: 650409472.0, Validation Loss: 709927040.0\n",
      "Epoch 169, Training Loss: 651469248.0, Validation Loss: 707236352.0\n",
      "Epoch 170, Training Loss: 644400000.0, Validation Loss: 704563840.0\n",
      "Epoch 171, Training Loss: 649132352.0, Validation Loss: 701903040.0\n",
      "Epoch 172, Training Loss: 643348352.0, Validation Loss: 699258368.0\n",
      "Epoch 173, Training Loss: 638917952.0, Validation Loss: 696640128.0\n",
      "Epoch 174, Training Loss: 636315584.0, Validation Loss: 694052608.0\n",
      "Epoch 175, Training Loss: 634293568.0, Validation Loss: 691492096.0\n",
      "Epoch 176, Training Loss: 630065088.0, Validation Loss: 688963072.0\n",
      "Epoch 177, Training Loss: 639196800.0, Validation Loss: 686445568.0\n",
      "Epoch 178, Training Loss: 629729920.0, Validation Loss: 683953792.0\n",
      "Epoch 179, Training Loss: 630216896.0, Validation Loss: 681481408.0\n",
      "Epoch 180, Training Loss: 625625344.0, Validation Loss: 679036096.0\n",
      "Epoch 181, Training Loss: 622031424.0, Validation Loss: 676617024.0\n",
      "Epoch 182, Training Loss: 616983552.0, Validation Loss: 674229312.0\n",
      "Epoch 183, Training Loss: 617926208.0, Validation Loss: 671864192.0\n",
      "Epoch 184, Training Loss: 613877056.0, Validation Loss: 669529664.0\n",
      "Epoch 185, Training Loss: 614691008.0, Validation Loss: 667220992.0\n",
      "Epoch 186, Training Loss: 609820480.0, Validation Loss: 664938880.0\n",
      "Epoch 187, Training Loss: 609146240.0, Validation Loss: 662681344.0\n",
      "Epoch 188, Training Loss: 606760960.0, Validation Loss: 660446272.0\n",
      "Epoch 189, Training Loss: 602709632.0, Validation Loss: 658235392.0\n",
      "Epoch 190, Training Loss: 600241856.0, Validation Loss: 656050624.0\n",
      "Epoch 191, Training Loss: 599072384.0, Validation Loss: 653889344.0\n",
      "Epoch 192, Training Loss: 598947840.0, Validation Loss: 651742016.0\n",
      "Epoch 193, Training Loss: 597951360.0, Validation Loss: 649607616.0\n",
      "Epoch 194, Training Loss: 596390784.0, Validation Loss: 647488576.0\n",
      "Epoch 195, Training Loss: 594475584.0, Validation Loss: 645382400.0\n",
      "Epoch 196, Training Loss: 589515584.0, Validation Loss: 643297856.0\n",
      "Epoch 197, Training Loss: 587715968.0, Validation Loss: 641231936.0\n",
      "Epoch 198, Training Loss: 584016448.0, Validation Loss: 639184896.0\n",
      "Epoch 199, Training Loss: 585028544.0, Validation Loss: 637151808.0\n",
      "Epoch 200, Training Loss: 582205760.0, Validation Loss: 635131072.0\n",
      "Epoch 201, Training Loss: 579846848.0, Validation Loss: 633124352.0\n",
      "Epoch 202, Training Loss: 576989952.0, Validation Loss: 631133056.0\n",
      "Epoch 203, Training Loss: 578367488.0, Validation Loss: 629152960.0\n",
      "Epoch 204, Training Loss: 574644608.0, Validation Loss: 627182528.0\n",
      "Epoch 205, Training Loss: 576019584.0, Validation Loss: 625215936.0\n",
      "Epoch 206, Training Loss: 570443072.0, Validation Loss: 623262272.0\n",
      "Epoch 207, Training Loss: 571276480.0, Validation Loss: 621313600.0\n",
      "Epoch 208, Training Loss: 568501312.0, Validation Loss: 619372864.0\n",
      "Epoch 209, Training Loss: 567432448.0, Validation Loss: 617438656.0\n",
      "Epoch 210, Training Loss: 562536640.0, Validation Loss: 615512896.0\n",
      "Epoch 211, Training Loss: 562958464.0, Validation Loss: 613591232.0\n",
      "Epoch 212, Training Loss: 559321984.0, Validation Loss: 611677248.0\n",
      "Epoch 213, Training Loss: 558406912.0, Validation Loss: 609768768.0\n",
      "Epoch 214, Training Loss: 557310336.0, Validation Loss: 607870976.0\n",
      "Epoch 215, Training Loss: 556579008.0, Validation Loss: 605975424.0\n",
      "Epoch 216, Training Loss: 554154048.0, Validation Loss: 604082240.0\n",
      "Epoch 217, Training Loss: 552583104.0, Validation Loss: 602196928.0\n",
      "Epoch 218, Training Loss: 549263232.0, Validation Loss: 600323008.0\n",
      "Epoch 219, Training Loss: 547697792.0, Validation Loss: 598458880.0\n",
      "Epoch 220, Training Loss: 545862208.0, Validation Loss: 596606016.0\n",
      "Epoch 221, Training Loss: 544433984.0, Validation Loss: 594762496.0\n",
      "Epoch 222, Training Loss: 546927616.0, Validation Loss: 592921792.0\n",
      "Epoch 223, Training Loss: 541089344.0, Validation Loss: 591091264.0\n",
      "Epoch 224, Training Loss: 538455104.0, Validation Loss: 589271296.0\n",
      "Epoch 225, Training Loss: 538245312.0, Validation Loss: 587456448.0\n",
      "Epoch 226, Training Loss: 534404032.0, Validation Loss: 585651008.0\n",
      "Epoch 227, Training Loss: 533792096.0, Validation Loss: 583853248.0\n",
      "Epoch 228, Training Loss: 534885632.0, Validation Loss: 582057984.0\n",
      "Epoch 229, Training Loss: 530552736.0, Validation Loss: 580271616.0\n",
      "Epoch 230, Training Loss: 530433600.0, Validation Loss: 578493632.0\n",
      "Epoch 231, Training Loss: 528769248.0, Validation Loss: 576723264.0\n",
      "Epoch 232, Training Loss: 527881792.0, Validation Loss: 574958976.0\n",
      "Epoch 233, Training Loss: 523609760.0, Validation Loss: 573203648.0\n",
      "Epoch 234, Training Loss: 521625824.0, Validation Loss: 571458368.0\n",
      "Epoch 235, Training Loss: 522393600.0, Validation Loss: 569717824.0\n",
      "Epoch 236, Training Loss: 518055136.0, Validation Loss: 567991872.0\n",
      "Epoch 237, Training Loss: 517883136.0, Validation Loss: 566276224.0\n",
      "Epoch 238, Training Loss: 514509152.0, Validation Loss: 564573504.0\n",
      "Epoch 239, Training Loss: 512168544.0, Validation Loss: 562884224.0\n",
      "Epoch 240, Training Loss: 512097248.0, Validation Loss: 561207488.0\n",
      "Epoch 241, Training Loss: 512997472.0, Validation Loss: 559534400.0\n",
      "Epoch 242, Training Loss: 506998016.0, Validation Loss: 557872640.0\n",
      "Epoch 243, Training Loss: 508827648.0, Validation Loss: 556217728.0\n",
      "Epoch 244, Training Loss: 504122240.0, Validation Loss: 554576640.0\n",
      "Epoch 245, Training Loss: 502807680.0, Validation Loss: 552949952.0\n",
      "Epoch 246, Training Loss: 502002752.0, Validation Loss: 551330112.0\n",
      "Epoch 247, Training Loss: 502165568.0, Validation Loss: 549711552.0\n",
      "Epoch 248, Training Loss: 498446048.0, Validation Loss: 548098496.0\n",
      "Epoch 249, Training Loss: 498566720.0, Validation Loss: 546487744.0\n",
      "Epoch 250, Training Loss: 495894208.0, Validation Loss: 544886016.0\n",
      "Epoch 251, Training Loss: 497211744.0, Validation Loss: 543279808.0\n",
      "Epoch 252, Training Loss: 494198784.0, Validation Loss: 541679680.0\n",
      "Epoch 253, Training Loss: 489673792.0, Validation Loss: 540086976.0\n",
      "Epoch 254, Training Loss: 490609504.0, Validation Loss: 538502592.0\n",
      "Epoch 255, Training Loss: 489851840.0, Validation Loss: 536928640.0\n",
      "Epoch 256, Training Loss: 487205184.0, Validation Loss: 535357856.0\n",
      "Epoch 257, Training Loss: 484833376.0, Validation Loss: 533796160.0\n",
      "Epoch 258, Training Loss: 484188064.0, Validation Loss: 532241632.0\n",
      "Epoch 259, Training Loss: 482278976.0, Validation Loss: 530693312.0\n",
      "Epoch 260, Training Loss: 480628512.0, Validation Loss: 529150272.0\n",
      "Epoch 261, Training Loss: 479230464.0, Validation Loss: 527614752.0\n",
      "Epoch 262, Training Loss: 477927136.0, Validation Loss: 526088288.0\n",
      "Epoch 263, Training Loss: 475658720.0, Validation Loss: 524569920.0\n",
      "Epoch 264, Training Loss: 476256000.0, Validation Loss: 523056448.0\n",
      "Epoch 265, Training Loss: 471549472.0, Validation Loss: 521556128.0\n",
      "Epoch 266, Training Loss: 472293216.0, Validation Loss: 520061440.0\n",
      "Epoch 267, Training Loss: 471414816.0, Validation Loss: 518571840.0\n",
      "Epoch 268, Training Loss: 470160832.0, Validation Loss: 517089056.0\n",
      "Epoch 269, Training Loss: 466114048.0, Validation Loss: 515611200.0\n",
      "Epoch 270, Training Loss: 467091840.0, Validation Loss: 514139904.0\n",
      "Epoch 271, Training Loss: 464570624.0, Validation Loss: 512671872.0\n",
      "Epoch 272, Training Loss: 464971072.0, Validation Loss: 511209312.0\n",
      "Epoch 273, Training Loss: 461814240.0, Validation Loss: 509754400.0\n",
      "Epoch 274, Training Loss: 460768192.0, Validation Loss: 508300000.0\n",
      "Epoch 275, Training Loss: 458499840.0, Validation Loss: 506853984.0\n",
      "Epoch 276, Training Loss: 457357888.0, Validation Loss: 505413216.0\n",
      "Epoch 277, Training Loss: 454632800.0, Validation Loss: 503982464.0\n",
      "Epoch 278, Training Loss: 455661472.0, Validation Loss: 502552384.0\n",
      "Epoch 279, Training Loss: 453478176.0, Validation Loss: 501126816.0\n",
      "Epoch 280, Training Loss: 453122336.0, Validation Loss: 499705792.0\n",
      "Epoch 281, Training Loss: 450195680.0, Validation Loss: 498292096.0\n",
      "Epoch 282, Training Loss: 448489248.0, Validation Loss: 496884448.0\n",
      "Epoch 283, Training Loss: 448685664.0, Validation Loss: 495484800.0\n",
      "Epoch 284, Training Loss: 449074592.0, Validation Loss: 494089568.0\n",
      "Epoch 285, Training Loss: 444878912.0, Validation Loss: 492706208.0\n",
      "Epoch 286, Training Loss: 443166624.0, Validation Loss: 491326592.0\n",
      "Epoch 287, Training Loss: 442134656.0, Validation Loss: 489952608.0\n",
      "Epoch 288, Training Loss: 442637312.0, Validation Loss: 488585408.0\n",
      "Epoch 289, Training Loss: 441440160.0, Validation Loss: 487225728.0\n",
      "Epoch 290, Training Loss: 437511232.0, Validation Loss: 485873248.0\n",
      "Epoch 291, Training Loss: 437414784.0, Validation Loss: 484522976.0\n",
      "Epoch 292, Training Loss: 436239104.0, Validation Loss: 483177824.0\n",
      "Epoch 293, Training Loss: 434640640.0, Validation Loss: 481843488.0\n",
      "Epoch 294, Training Loss: 430856800.0, Validation Loss: 480521920.0\n",
      "Epoch 295, Training Loss: 431558976.0, Validation Loss: 479202624.0\n",
      "Epoch 296, Training Loss: 429839104.0, Validation Loss: 477889440.0\n",
      "Epoch 297, Training Loss: 427579072.0, Validation Loss: 476586784.0\n",
      "Epoch 298, Training Loss: 429119104.0, Validation Loss: 475288256.0\n",
      "Epoch 299, Training Loss: 426603936.0, Validation Loss: 473998816.0\n",
      "Epoch 300, Training Loss: 424397184.0, Validation Loss: 472712928.0\n",
      "Epoch 301, Training Loss: 422657344.0, Validation Loss: 471424736.0\n",
      "Epoch 302, Training Loss: 422015776.0, Validation Loss: 470136992.0\n",
      "Epoch 303, Training Loss: 422969888.0, Validation Loss: 468852736.0\n",
      "Epoch 304, Training Loss: 419686112.0, Validation Loss: 467576224.0\n",
      "Epoch 305, Training Loss: 419184000.0, Validation Loss: 466302688.0\n",
      "Epoch 306, Training Loss: 416123648.0, Validation Loss: 465037408.0\n",
      "Epoch 307, Training Loss: 417598080.0, Validation Loss: 463772000.0\n",
      "Epoch 308, Training Loss: 414500192.0, Validation Loss: 462514784.0\n",
      "Epoch 309, Training Loss: 413336544.0, Validation Loss: 461263168.0\n",
      "Epoch 310, Training Loss: 412557984.0, Validation Loss: 460012384.0\n",
      "Epoch 311, Training Loss: 410362272.0, Validation Loss: 458766688.0\n",
      "Epoch 312, Training Loss: 410421184.0, Validation Loss: 457523488.0\n",
      "Epoch 313, Training Loss: 409067392.0, Validation Loss: 456279232.0\n",
      "Epoch 314, Training Loss: 407391104.0, Validation Loss: 455037536.0\n",
      "Epoch 315, Training Loss: 408905696.0, Validation Loss: 453798592.0\n",
      "Epoch 316, Training Loss: 404425088.0, Validation Loss: 452566688.0\n",
      "Epoch 317, Training Loss: 401668448.0, Validation Loss: 451348992.0\n",
      "Epoch 318, Training Loss: 402038624.0, Validation Loss: 450136192.0\n",
      "Epoch 319, Training Loss: 402977824.0, Validation Loss: 448925728.0\n",
      "Epoch 320, Training Loss: 400645408.0, Validation Loss: 447722112.0\n",
      "Epoch 321, Training Loss: 398762208.0, Validation Loss: 446528864.0\n",
      "Epoch 322, Training Loss: 397292128.0, Validation Loss: 445342368.0\n",
      "Epoch 323, Training Loss: 396059296.0, Validation Loss: 444168480.0\n",
      "Epoch 324, Training Loss: 394306336.0, Validation Loss: 442999008.0\n",
      "Epoch 325, Training Loss: 394600416.0, Validation Loss: 441835104.0\n",
      "Epoch 326, Training Loss: 393237824.0, Validation Loss: 440676416.0\n",
      "Epoch 327, Training Loss: 393692960.0, Validation Loss: 439519648.0\n",
      "Epoch 328, Training Loss: 389818592.0, Validation Loss: 438372576.0\n",
      "Epoch 329, Training Loss: 390561056.0, Validation Loss: 437231232.0\n",
      "Epoch 330, Training Loss: 387752512.0, Validation Loss: 436096128.0\n",
      "Epoch 331, Training Loss: 386531872.0, Validation Loss: 434970912.0\n",
      "Epoch 332, Training Loss: 385232480.0, Validation Loss: 433850752.0\n",
      "Epoch 333, Training Loss: 387219936.0, Validation Loss: 432727552.0\n",
      "Epoch 334, Training Loss: 384428448.0, Validation Loss: 431607968.0\n",
      "Epoch 335, Training Loss: 383017152.0, Validation Loss: 430493824.0\n",
      "Epoch 336, Training Loss: 380885920.0, Validation Loss: 429381792.0\n",
      "Epoch 337, Training Loss: 379977536.0, Validation Loss: 428279232.0\n",
      "Epoch 338, Training Loss: 380337984.0, Validation Loss: 427180384.0\n",
      "Epoch 339, Training Loss: 379298144.0, Validation Loss: 426080768.0\n",
      "Epoch 340, Training Loss: 377569312.0, Validation Loss: 424987360.0\n",
      "Epoch 341, Training Loss: 376097088.0, Validation Loss: 423902656.0\n",
      "Epoch 342, Training Loss: 375071168.0, Validation Loss: 422824960.0\n",
      "Epoch 343, Training Loss: 375142816.0, Validation Loss: 421751296.0\n",
      "Epoch 344, Training Loss: 373887872.0, Validation Loss: 420682240.0\n",
      "Epoch 345, Training Loss: 370535904.0, Validation Loss: 419628864.0\n",
      "Epoch 346, Training Loss: 370199744.0, Validation Loss: 418587488.0\n",
      "Epoch 347, Training Loss: 369932960.0, Validation Loss: 417547040.0\n",
      "Epoch 348, Training Loss: 368179424.0, Validation Loss: 416515808.0\n",
      "Epoch 349, Training Loss: 368362784.0, Validation Loss: 415491936.0\n",
      "Epoch 350, Training Loss: 365971200.0, Validation Loss: 414476512.0\n",
      "Epoch 351, Training Loss: 367106624.0, Validation Loss: 413467168.0\n",
      "Epoch 352, Training Loss: 363961536.0, Validation Loss: 412467744.0\n",
      "Epoch 353, Training Loss: 364145536.0, Validation Loss: 411474688.0\n",
      "Epoch 354, Training Loss: 362706304.0, Validation Loss: 410481408.0\n",
      "Epoch 355, Training Loss: 361639744.0, Validation Loss: 409496288.0\n",
      "Epoch 356, Training Loss: 360674304.0, Validation Loss: 408511232.0\n",
      "Epoch 357, Training Loss: 360336192.0, Validation Loss: 407528896.0\n",
      "Epoch 358, Training Loss: 356784032.0, Validation Loss: 406558432.0\n",
      "Epoch 359, Training Loss: 356816224.0, Validation Loss: 405593376.0\n",
      "Epoch 360, Training Loss: 357448416.0, Validation Loss: 404631520.0\n",
      "Epoch 361, Training Loss: 356409568.0, Validation Loss: 403671712.0\n",
      "Epoch 362, Training Loss: 353475872.0, Validation Loss: 402722464.0\n",
      "Epoch 363, Training Loss: 353117824.0, Validation Loss: 401778752.0\n",
      "Epoch 364, Training Loss: 352494784.0, Validation Loss: 400839840.0\n",
      "Epoch 365, Training Loss: 353298304.0, Validation Loss: 399901696.0\n",
      "Epoch 366, Training Loss: 351568128.0, Validation Loss: 398969216.0\n",
      "Epoch 367, Training Loss: 349514496.0, Validation Loss: 398042752.0\n",
      "Epoch 368, Training Loss: 348795360.0, Validation Loss: 397114752.0\n",
      "Epoch 369, Training Loss: 347875552.0, Validation Loss: 396193248.0\n",
      "Epoch 370, Training Loss: 346754400.0, Validation Loss: 395276320.0\n",
      "Epoch 371, Training Loss: 344985600.0, Validation Loss: 394370720.0\n",
      "Epoch 372, Training Loss: 345737632.0, Validation Loss: 393470080.0\n",
      "Epoch 373, Training Loss: 343250432.0, Validation Loss: 392578080.0\n",
      "Epoch 374, Training Loss: 343297408.0, Validation Loss: 391689568.0\n",
      "Epoch 375, Training Loss: 342203072.0, Validation Loss: 390799104.0\n",
      "Epoch 376, Training Loss: 342359776.0, Validation Loss: 389908768.0\n",
      "Epoch 377, Training Loss: 340404672.0, Validation Loss: 389019488.0\n",
      "Epoch 378, Training Loss: 339324704.0, Validation Loss: 388135424.0\n",
      "Epoch 379, Training Loss: 338228000.0, Validation Loss: 387257344.0\n",
      "Epoch 380, Training Loss: 337737440.0, Validation Loss: 386389216.0\n",
      "Epoch 381, Training Loss: 338452928.0, Validation Loss: 385520224.0\n",
      "Epoch 382, Training Loss: 335507104.0, Validation Loss: 384662080.0\n",
      "Epoch 383, Training Loss: 334904096.0, Validation Loss: 383817056.0\n",
      "Epoch 384, Training Loss: 335236288.0, Validation Loss: 382974144.0\n",
      "Epoch 385, Training Loss: 333683744.0, Validation Loss: 382139648.0\n",
      "Epoch 386, Training Loss: 332431008.0, Validation Loss: 381310528.0\n",
      "Epoch 387, Training Loss: 331264192.0, Validation Loss: 380493056.0\n",
      "Epoch 388, Training Loss: 331384800.0, Validation Loss: 379685056.0\n",
      "Epoch 389, Training Loss: 328121504.0, Validation Loss: 378889856.0\n",
      "Epoch 390, Training Loss: 329657280.0, Validation Loss: 378099264.0\n",
      "Epoch 391, Training Loss: 327234816.0, Validation Loss: 377313760.0\n",
      "Epoch 392, Training Loss: 327771392.0, Validation Loss: 376532000.0\n",
      "Epoch 393, Training Loss: 326251904.0, Validation Loss: 375755744.0\n",
      "Epoch 394, Training Loss: 325574144.0, Validation Loss: 374985344.0\n",
      "Epoch 395, Training Loss: 325266080.0, Validation Loss: 374213792.0\n",
      "Epoch 396, Training Loss: 324030592.0, Validation Loss: 373449216.0\n",
      "Epoch 397, Training Loss: 323542912.0, Validation Loss: 372685920.0\n",
      "Epoch 398, Training Loss: 322940736.0, Validation Loss: 371931072.0\n",
      "Epoch 399, Training Loss: 323178432.0, Validation Loss: 371184160.0\n",
      "Epoch 400, Training Loss: 319316448.0, Validation Loss: 370445600.0\n",
      "Epoch 401, Training Loss: 319569120.0, Validation Loss: 369712320.0\n",
      "Epoch 402, Training Loss: 320272608.0, Validation Loss: 368983648.0\n",
      "Epoch 403, Training Loss: 318374272.0, Validation Loss: 368261408.0\n",
      "Epoch 404, Training Loss: 317290240.0, Validation Loss: 367544192.0\n",
      "Epoch 405, Training Loss: 317685408.0, Validation Loss: 366829504.0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(Data(x=x_tensor_train, edge_index=edge_index_train))\n",
    "    loss = F.mse_loss(out, y_tensor_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(Data(x=x_tensor_val, edge_index=edge_index_val))\n",
    "        val_loss = F.mse_loss(val_out, y_tensor_val)\n",
    "\n",
    "    print(f'Epoch {epoch}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "    # Обновление скорости обучения\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early Stopping проверка\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
